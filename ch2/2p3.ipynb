{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Linear Algebra\n",
    "\n",
    "2.3.13 Exercises\n",
    "\n",
    "1 2 and 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./2p3_123.jpeg\" width = \"1000\" height = \"1200\" alt=\"2p3_123\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We defined the tensor X of shape (2, 3, 4) in this section. What is the output of len(X)? Write your answer without implementing any code, then check your answer using code.\n",
    "5. For a tensor X of arbitrary shape, does len(X) always correspond to the length of a certain axis of X? What is that axis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "y = torch.arange(24).reshape(4, 3, 2)\n",
    "print(len(X))\n",
    "print(len(y)) # len() will output the first dimension of the tensor, which is 2 in this case.\n",
    "print(X.shape) # shape() will output the shape of the tensor, which is (2, 3, 4) in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Run A / A.sum(axis=1) and see what happens. Can you analyze the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n",
      "tensor([ 3., 12.])\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "print(A)\n",
    "print(A.sum(axis=1)) # sum along axis 1, length of dimension 1 will become 1\n",
    "# print(A / A.sum(axis=1)) # do not fulfill the requirement of broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. When traveling between two points in downtown Manhattan, what is the distance that you need to cover in terms of the coordinates, i.e., in terms of avenues and streets? Can you travel diagonally?\n",
    "   \n",
    "    No, you cannot travel diagonally between two points in downtown Manhattan. The distance that you need to cover between two points in Manhattan is calculated through l1 norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Consider a tensor of shape (2, 3, 4). What are the shapes of the summation outputs along axes 0, 1, and 2?\n",
    "\n",
    "    The shapes of the summation outputs along axes 0, 1, and 2 are:\n",
    "\n",
    "    - Output along axis 0: (1, 3, 4) if keepdim=True, (3, 4) otherwise.\n",
    "    - Output along axis 1: (2, 1, 4) if keepdim=True, (2, 4) otherwise.\n",
    "    - Output along axis 2: (2, 3, 1) if keepdim=True, (2, 3) otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([1, 3, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 1, 4])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X.sum(axis=0).shape)\n",
    "print(X.sum(axis=0, keepdim=True).shape)\n",
    "print(X.sum(axis=1).shape)\n",
    "print(X.sum(axis=1, keepdim=True).shape)\n",
    "print(X.sum(axis=2).shape)\n",
    "print(X.sum(axis=2, keepdim=True).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Feed a tensor with three or more axes to the linalg.norm function and observe its output. What does this function compute for tensors of arbitrary shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.8322)\n",
      "tensor(11.8322)\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "X1 = torch.arange(8, dtype=torch.float32).reshape(2, 2, 2)\n",
    "# print(X1.dtype) # dtype of int64 is not valid for linalg.norm\n",
    "print(torch.linalg.norm(X1)) # the result is the square root of the sum of squares of all elements in X1.\n",
    "                             # the calculation mechanism is the same as l2 norm of vector.\n",
    "print(torch.norm(X1)) # same as torch.linalg.norm(X1)\n",
    "\n",
    "y1 = torch.norm(X1, dim=1) # calc norm along dim1, length of dim1 become 1\n",
    "print(y1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Define three large matrices, say $ \\mathbf{A} \\in \\mathbb{R}^{2^{10} \\times 2^{16}} $, $ \\mathbf{B} \\in \\mathbb{R}^{2^{16} \\times 2^{5}} $ and $ \\mathbf{C} \\in \\mathbb{R}^{2^{5} \\times 2^{16}} $, for instance initialized with Gaussian random variables. You want to compute the product . Is there any difference in memory footprint and speed, depending on whether you compute $ (\\mathbf{A} \\mathbf{B}) \\mathbf{C} $ or $ \\mathbf{A} (\\mathbf{B} \\mathbf{C}) $. Why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 65536])\n",
      "torch.Size([65536, 32])\n",
      "torch.Size([32, 16384])\n",
      "Time taken: 0.018867015838623047 seconds\n",
      "Time taken: 3.390151262283325 seconds\n"
     ]
    }
   ],
   "source": [
    "A = torch.randn(pow(2,10), pow(2,16))\n",
    "print(A.shape)\n",
    "B = torch.randn(pow(2,16), pow(2,5))\n",
    "print(B.shape)\n",
    "C = torch.randn(pow(2,5), pow(2,14))\n",
    "print(C.shape)\n",
    "\n",
    "# (AB)C\n",
    "time_start = time.time()\n",
    "D = torch.matmul(torch.matmul(A, B), C)\n",
    "time_end = time.time()\n",
    "print(\"Time taken:\", time_end - time_start, \"seconds\")\n",
    "\n",
    "# (A(BC))\n",
    "time_start = time.time()\n",
    "D = torch.matmul(A, torch.matmul(B, C))\n",
    "time_end = time.time()\n",
    "print(\"Time taken:\", time_end - time_start, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ (\\mathbf{A} \\mathbf{B}) \\mathbf{C} $ will result in time being $ \\mathcal{O}(2^{10} \\times 2^{16} \\times 2^5 + 2^{10} \\times 2^{5} \\times 2^{16} ) $\n",
    "\n",
    "$ \\mathbf{A} (\\mathbf{B} \\mathbf{C}) $ will result in time being $ \\mathcal{O}(2^{10} \\times 2^{16} \\times 2^{16} + 2^{16} \\times 2^{5} \\times 2^{16} ) $\n",
    "\n",
    "The latter is much much more expensive because of the algorithms being designed to have complexity of $ \\mathcal{O}(n \\times m \\times k) $ where the matrices $ A $ and $ B $ are of size $ \\mathbb{R}^{n \\times m} $ and $ \\mathbb{R}^{m \\times k} $ respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Define three large matrices, say $ \\mathbf{A} \\in \\mathbb{R}^{2^{10} \\times 2^{16}} $ , $ \\mathbf{B} \\in \\mathbb{R}^{2^{16} \\times 2^{5}} $ and $ \\mathbf{C} \\in \\mathbb{R}^{2^{5} \\times 2^{16}} $. Is there any difference in speed depending on whether you compute $ \\mathbf{A} \\mathbf{B} $ or $ \\mathbf{A} \\mathbf{C}^\\top $? Why? What changes if you initialize $ \\mathbf{C} = \\mathbf{B}^\\top $ without cloning memory? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 65536])\n",
      "torch.Size([65536, 32])\n",
      "torch.Size([32, 65536])\n",
      "AB:  0.010829687118530273 s\n",
      "A(C.transpose()):  0.011614084243774414 s\n"
     ]
    }
   ],
   "source": [
    "A = torch.randn(pow(2,10), pow(2,16))\n",
    "print(A.shape)\n",
    "B = torch.randn(pow(2,16), pow(2,5))\n",
    "print(B.shape)\n",
    "C = torch.randn(pow(2,5), pow(2,16))\n",
    "print(C.shape)\n",
    "# C = B.detach().transpose(0, 1)\n",
    "\n",
    "# AB\n",
    "time_start = time.time()\n",
    "AB = torch.matmul(A, B)\n",
    "time_end = time.time()\n",
    "print(\"AB: \", time_end - time_start, 's')\n",
    "\n",
    "# A(C.trnaspose())\n",
    "time_start = time.time()\n",
    "A_C_transpose = torch.matmul(A, C.transpose(0, 1))\n",
    "time_end = time.time()\n",
    "print(\"A(C.transpose()): \", time_end - time_start, 's')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\mathbf{A} \\times \\mathbf{C}^{\\top} $ yields better performance due to the layout of data in memory : since the row major format in which data is usually stored in torch usually prefers memory accesses of the same row, when you take transpose of $ \\mathbf{C}^{\\top} $, it’s not really taking a physical transpose, but a logical one, meaning when we index the trasposes matrix at $ (i, j) $, it just gets internally converted to $ (j, i) $ of the matrix before transposition. Since the elements of the second matrix are accessed column-wise, it is inefficient for this task, but if we have it logically transposed, then the accesses become efficent again, since the data is logically being accessed across rows, ie, columns-wise, but is physically geting accessed across columns, ie, row-wise, since we didn’t actually perform the element swaps, only decided to change indexing under the hood. Hence, the transpose technique works faster\n",
    "元素的访问顺序(列优先or行优先)也会造成运算速度上的差别。\n",
    "\n",
    "didn't find any changes after initializing $\\mathbf{C} = \\mathbf{B}^\\top$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Define three matrices, say $ \\mathbf{A}, \\mathbf{B}, \\mathbf{C} \\in \\mathbb{R}^{100 \\times 200} $. Constitute a tensor with 3 axes by stacking $ [\\mathbf{A}, \\mathbf{B}, \\mathbf{C}] $ . What is the dimensionality? Slice out the second coordinate of the third axis to recover $\\mathbf{B} $ . Check that your answer is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial B: tensor([[0.1770, 0.1373, 0.9343,  ..., 0.1136, 0.5935, 0.5402],\n",
      "        [0.0676, 0.2028, 0.9920,  ..., 0.6709, 0.5153, 0.2509],\n",
      "        [0.0893, 0.0176, 0.2141,  ..., 0.0090, 0.3610, 0.7305],\n",
      "        ...,\n",
      "        [0.4872, 0.9601, 0.7155,  ..., 0.6087, 0.4426, 0.2300],\n",
      "        [0.5169, 0.8145, 0.3190,  ..., 0.3515, 0.7946, 0.8816],\n",
      "        [0.1129, 0.5145, 0.1850,  ..., 0.1536, 0.5530, 0.3248]])\n",
      "stacked: tensor([[[0.6426, 0.6216, 0.7336,  ..., 0.4522, 0.1269, 0.6817],\n",
      "         [0.3433, 0.2755, 0.6361,  ..., 0.9157, 0.7919, 0.0993],\n",
      "         [0.7901, 0.0534, 0.2217,  ..., 0.7983, 0.8121, 0.1477],\n",
      "         ...,\n",
      "         [0.1409, 0.2452, 0.9959,  ..., 0.0080, 0.0974, 0.5359],\n",
      "         [0.9740, 0.9278, 0.5043,  ..., 0.7033, 0.5929, 0.4846],\n",
      "         [0.4480, 0.7080, 0.9999,  ..., 0.1651, 0.4944, 0.9958]],\n",
      "\n",
      "        [[0.1770, 0.1373, 0.9343,  ..., 0.1136, 0.5935, 0.5402],\n",
      "         [0.0676, 0.2028, 0.9920,  ..., 0.6709, 0.5153, 0.2509],\n",
      "         [0.0893, 0.0176, 0.2141,  ..., 0.0090, 0.3610, 0.7305],\n",
      "         ...,\n",
      "         [0.4872, 0.9601, 0.7155,  ..., 0.6087, 0.4426, 0.2300],\n",
      "         [0.5169, 0.8145, 0.3190,  ..., 0.3515, 0.7946, 0.8816],\n",
      "         [0.1129, 0.5145, 0.1850,  ..., 0.1536, 0.5530, 0.3248]],\n",
      "\n",
      "        [[0.7161, 0.3218, 0.8514,  ..., 0.9959, 0.8049, 0.3208],\n",
      "         [0.6358, 0.0171, 0.1856,  ..., 0.6660, 0.3528, 0.1226],\n",
      "         [0.7766, 0.9774, 0.0403,  ..., 0.3495, 0.3201, 0.5417],\n",
      "         ...,\n",
      "         [0.1439, 0.5572, 0.4118,  ..., 0.6956, 0.1921, 0.2224],\n",
      "         [0.1658, 0.4225, 0.2640,  ..., 0.0709, 0.1297, 0.0254],\n",
      "         [0.3417, 0.9903, 0.0299,  ..., 0.6353, 0.8762, 0.4747]]])\n",
      "sliced B: tensor([[0.1770, 0.1373, 0.9343,  ..., 0.1136, 0.5935, 0.5402],\n",
      "        [0.0676, 0.2028, 0.9920,  ..., 0.6709, 0.5153, 0.2509],\n",
      "        [0.0893, 0.0176, 0.2141,  ..., 0.0090, 0.3610, 0.7305],\n",
      "        ...,\n",
      "        [0.4872, 0.9601, 0.7155,  ..., 0.6087, 0.4426, 0.2300],\n",
      "        [0.5169, 0.8145, 0.3190,  ..., 0.3515, 0.7946, 0.8816],\n",
      "        [0.1129, 0.5145, 0.1850,  ..., 0.1536, 0.5530, 0.3248]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand(100, 200)\n",
    "B = torch.rand(100, 200)\n",
    "C = torch.rand(100, 200)\n",
    "print('initial B:', B)\n",
    "stacked = torch.stack([A, B, C], dim=0) # 新生成的维度在dim0处\n",
    "print('stacked:', stacked)\n",
    "\n",
    "B = stacked[1]\n",
    "print('sliced B:', B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thanks to Tejas-​​Garhewal for his attempts of 2.3 exercises."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
