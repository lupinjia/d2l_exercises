{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6 Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. When can you solve the problem of polynomial regression exactly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $n$ data samples with distinct x values, samppled from a $m$-order polynomial function, a $d$-degree $(d>=m)$ polynomial model can solve the problem exactly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Give at least five examples where dependent random variables make treating the problem as IID data inadvisable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Can you ever expect to see zero training error? Under which circumstances would you see zero generalization error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, for very simple linear problem without noise, the training error may appear to be zero, and the generalization error may also be zero.(because the problem is too simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Why is $K$-fold cross-validation very expensive to compute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the normal way(where the training set and validation set are fixed), we only need to iterate the (training+validation) set once. But for K-fold cross-validation, we need to iterate the (training+validation) set K times, so it's more time-consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Why is the $K$-fold cross-validation error estimate biased?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the validation set has actually been seen by your model when using K-fold cross-validation. The validation error can't represent your model's generalization ability perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The VC dimension is defined as the maximum number of points that can be classified with arbitrary labels $\\{\\pm 1\\}$ by a function of a class of functions. Why might this not be a good idea for measuring how complex the class of functions is? Hint: consider the magnitude of the functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Your manager gives you a difficult dataset on which your current algorithm does not perform so well. How would you justify to him that you need more data? Hint: you cannot increase the data but you can decrease it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show him/her the process of training/validation error decreasing as the data increases. Just make use of the current data, to train your model on part of them and increase the data gradually until reach the full amount of the data. Plot the curve of error w.r.t. data amount."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
